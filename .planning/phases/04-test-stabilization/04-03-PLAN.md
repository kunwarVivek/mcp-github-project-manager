---
phase: 04-test-stabilization
plan: 03
type: execute
wave: 2
depends_on: [04-01, 04-02]
files_modified:
  - src/__tests__/unit/context/ContextualReferenceGenerator.test.ts
autonomous: true

must_haves:
  truths:
    - "ContextualReferenceGenerator has unit tests covering all public methods"
    - "AI-available and AI-unavailable code paths are both tested"
    - "Error handling paths are tested"
    - "Coverage on ContextualReferenceGenerator is 90%+"
  artifacts:
    - path: "src/__tests__/unit/context/ContextualReferenceGenerator.test.ts"
      provides: "Comprehensive unit tests for ContextualReferenceGenerator"
      min_lines: 200
  key_links:
    - from: "test file"
      to: "ContextualReferenceGenerator"
      via: "import"
      pattern: "import.*ContextualReferenceGenerator"
---

<objective>
Add comprehensive unit tests for ContextualReferenceGenerator service.

Purpose: This service (407 lines) currently has zero test coverage. It generates contextual references for tasks using AI when available and fallback logic otherwise. Per DEBT-16, this needs 90%+ coverage.

Output: ContextualReferenceGenerator.test.ts with comprehensive tests covering all code paths.
</objective>

<execution_context>
@/Users/vivek/.claude/get-shit-done/workflows/execute-plan.md
@/Users/vivek/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-test-stabilization/04-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ContextualReferenceGenerator test file</name>
  <files>src/__tests__/unit/context/ContextualReferenceGenerator.test.ts</files>
  <action>
Create the test file following the established AI mocking pattern from RESEARCH.md.

First, create the directory if needed:
```bash
mkdir -p src/__tests__/unit/context
```

Create the test file with this structure:

```typescript
import { describe, it, expect, beforeEach, jest } from '@jest/globals';
import { ContextualReferenceGenerator } from '../../../services/context/ContextualReferenceGenerator';
import { AIServiceFactory } from '../../../services/ai/AIServiceFactory';
import { AITask, PRDDocument, TaskStatus, TaskPriority, TaskComplexity } from '../../../domain/ai-types';

jest.mock('../../../services/ai/AIServiceFactory');
jest.mock('ai', () => ({
  generateObject: jest.fn()
}));

describe('ContextualReferenceGenerator', () => {
  let generator: ContextualReferenceGenerator;
  let mockFactory: any;

  // Mock data - use realistic test fixtures
  const mockTask: AITask = {
    id: 'task-1',
    title: 'Implement user authentication',
    description: 'Create secure login functionality with OAuth support',
    status: TaskStatus.PENDING,
    priority: TaskPriority.HIGH,
    complexity: 7 as TaskComplexity,
    estimatedHours: 16,
    actualHours: 0,
    aiGenerated: true,
    subtasks: [],
    dependencies: [],
    acceptanceCriteria: [
      { id: 'ac-1', description: 'Users can login with email/password', completed: false }
    ],
    createdAt: new Date().toISOString(),
    updatedAt: new Date().toISOString(),
    tags: ['auth', 'security']
  };

  const mockPRD: PRDDocument = {
    id: 'prd-1',
    title: 'Authentication System',
    version: '1.0',
    overview: 'Secure authentication for the platform',
    objectives: ['Secure user authentication', 'Support OAuth providers'],
    scope: {
      inScope: ['Login', 'Registration', 'Password reset'],
      outOfScope: ['2FA'],
      assumptions: ['Users have email'],
      constraints: ['Must work on mobile']
    },
    targetUsers: [],
    userJourney: 'User registers, verifies email, logs in',
    features: [
      {
        id: 'f-1',
        title: 'User Login',
        description: 'Email/password authentication',
        userStories: ['As a user I can login'],
        businessValue: 'Core functionality',
        priority: 'high' as const,
        complexity: 'medium' as const,
        acceptanceCriteria: ['Login works with valid credentials']
      }
    ],
    technicalRequirements: ['JWT tokens', 'bcrypt hashing'],
    timeline: '3 months',
    milestones: ['MVP'],
    successMetrics: ['99% login success rate'],
    aiGenerated: true,
    createdAt: new Date().toISOString(),
    updatedAt: new Date().toISOString(),
    author: 'test',
    stakeholders: ['engineering'],
    tags: ['auth']
  };

  beforeEach(() => {
    jest.clearAllMocks();

    mockFactory = {
      getBestAvailableModel: jest.fn().mockReturnValue({ modelId: 'test-model' }),
      getMainModel: jest.fn().mockReturnValue({ modelId: 'test-model' }),
      getFallbackModel: jest.fn().mockReturnValue({ modelId: 'fallback-model' })
    };

    (AIServiceFactory.getInstance as jest.Mock).mockReturnValue(mockFactory);

    generator = new ContextualReferenceGenerator();
  });

  describe('generateReferences', () => {
    it('should generate references with AI when available', async () => {
      const { generateObject } = require('ai');
      generateObject.mockResolvedValue({
        object: {
          prdSections: [{ sectionTitle: 'Overview', relevance: 'high', content: 'Auth overview' }],
          relatedFeatures: [{ featureId: 'f-1', title: 'User Login', relationship: 'implements' }],
          technicalSpecs: [{ specType: 'security', specification: 'Use JWT' }],
          codeExamples: [],
          externalReferences: []
        }
      });

      const result = await generator.generateReferences(mockTask, mockPRD);

      expect(result).toBeDefined();
      expect(generateObject).toHaveBeenCalled();
      expect(result?.prdSections).toHaveLength(1);
      expect(result?.relatedFeatures).toHaveLength(1);
    });

    it('should use fallback when AI is unavailable', async () => {
      mockFactory.getBestAvailableModel.mockReturnValue(null);
      generator = new ContextualReferenceGenerator();

      const result = await generator.generateReferences(mockTask, mockPRD);

      expect(result).toBeDefined();
      expect(result?.prdSections).toBeDefined();
      // Fallback should extract from PRD directly
    });

    it('should handle AI errors gracefully', async () => {
      const { generateObject } = require('ai');
      generateObject.mockRejectedValue(new Error('AI service error'));

      const result = await generator.generateReferences(mockTask, mockPRD);

      // Should fall back to basic references instead of throwing
      expect(result).toBeDefined();
    });

    it('should handle null PRD', async () => {
      const result = await generator.generateReferences(mockTask, null as any);

      // Should return empty/minimal references
      expect(result).toBeDefined();
    });

    it('should handle empty task description', async () => {
      const emptyTask = { ...mockTask, description: '' };

      const result = await generator.generateReferences(emptyTask, mockPRD);

      expect(result).toBeDefined();
    });
  });

  // Add more describe blocks for other public methods
  // based on what the service actually exposes
});
```

Read the actual ContextualReferenceGenerator.ts to understand:
1. All public methods that need testing
2. The actual return types
3. Edge cases to cover

Then expand tests to achieve 90%+ coverage.
  </action>
  <verify>
Run tests with coverage:
```bash
npm test -- --testPathPattern="ContextualReferenceGenerator.test" --coverage --collectCoverageFrom="src/services/context/ContextualReferenceGenerator.ts" 2>&1 | grep -E "(PASS|FAIL|Coverage|ContextualReferenceGenerator)"
```

Target: 90%+ coverage on ContextualReferenceGenerator.ts
  </verify>
  <done>ContextualReferenceGenerator.test.ts exists with 90%+ coverage on the source file.</done>
</task>

<task type="auto">
  <name>Task 2: Add edge case and fallback path tests</name>
  <files>src/__tests__/unit/context/ContextualReferenceGenerator.test.ts</files>
  <action>
Expand the test file with additional test cases to cover:

1. **Fallback paths** (when AI unavailable):
   - Test that fallback extracts PRD sections by keyword matching
   - Test that fallback identifies related features by title similarity
   - Test that fallback returns minimal but valid structure

2. **Error handling**:
   - AI timeout errors
   - Malformed AI responses
   - Missing required fields in task/PRD

3. **Edge cases**:
   - Task with no tags
   - PRD with no features array
   - PRD with empty objectives
   - Very long descriptions (token limits)

4. **Input variations**:
   - String PRD (JSON string) vs PRDDocument object
   - Task with subtasks
   - Task with dependencies

Add tests like:
```typescript
describe('fallback behavior', () => {
  beforeEach(() => {
    mockFactory.getBestAvailableModel.mockReturnValue(null);
    generator = new ContextualReferenceGenerator();
  });

  it('should extract PRD sections matching task keywords', async () => {
    const result = await generator.generateReferences(mockTask, mockPRD);

    // Fallback should find 'authentication' in PRD overview
    expect(result?.prdSections.some(s =>
      s.content.toLowerCase().includes('auth')
    )).toBe(true);
  });

  it('should identify related features by title similarity', async () => {
    const result = await generator.generateReferences(mockTask, mockPRD);

    // Should match 'User Login' feature with 'authentication' task
    expect(result?.relatedFeatures).toBeDefined();
  });
});

describe('error recovery', () => {
  it('should handle timeout errors', async () => {
    const { generateObject } = require('ai');
    generateObject.mockRejectedValue(new Error('timeout'));

    const result = await generator.generateReferences(mockTask, mockPRD);

    expect(result).toBeDefined();
    // Should not throw
  });

  it('should handle malformed AI response', async () => {
    const { generateObject } = require('ai');
    generateObject.mockResolvedValue({ object: null });

    const result = await generator.generateReferences(mockTask, mockPRD);

    expect(result).toBeDefined();
  });
});
```
  </action>
  <verify>
Run coverage again:
```bash
npm test -- --testPathPattern="ContextualReferenceGenerator.test" --coverage --collectCoverageFrom="src/services/context/ContextualReferenceGenerator.ts" 2>&1 | grep -E "Stmts|Branch|Lines"
```

All metrics should be 90%+.
  </verify>
  <done>ContextualReferenceGenerator has comprehensive test coverage including fallback paths and error handling.</done>
</task>

</tasks>

<verification>
Run focused test and coverage:
```bash
npm test -- --testPathPattern="ContextualReferenceGenerator" --coverage --collectCoverageFrom="src/services/context/ContextualReferenceGenerator.ts" 2>&1
```

Expected:
- All tests pass
- Statement coverage >= 90%
- Branch coverage >= 85%
- Line coverage >= 90%
</verification>

<success_criteria>
1. ContextualReferenceGenerator.test.ts exists with 15+ test cases
2. AI-available path is tested
3. AI-unavailable fallback path is tested
4. Error handling is tested
5. Coverage metrics: 90%+ statements, 85%+ branches
</success_criteria>

<output>
After completion, create `.planning/phases/04-test-stabilization/04-03-SUMMARY.md`
</output>
